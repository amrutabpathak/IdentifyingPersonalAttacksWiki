{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of attacks in Wikipedia comments\n",
    "\n",
    "## Approach \n",
    "\n",
    "### Initial Code\n",
    "The strawman code was used initally and then it was modified as per the need. The comments_annotation and comments file was downloaded to the machine. \n",
    "\n",
    "*Please note that since data is imbalanced F1 Score was reported for 'attack' label. It is note worthy that F1 score for all 'not attack' label was greater than 0.9 for all the experiments.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from stemmer import StemTokenizer\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')\n",
    "\n",
    "len(annotations['rev_id'].unique())\n",
    "\n",
    "# labels a comment as an atack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5\n",
    "\n",
    "# join labels and comments\n",
    "comments['attack'] = labels\n",
    "\n",
    "# remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "\n",
    "# fit a simple text classifier\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Baseline\n",
    "Running the given strawman code resulted in following metrics.\n",
    "\n",
    "Test ROC AUC: 0.957\n",
    "\n",
    "Precison and recall:\n",
    "\n",
    "    True Positive: (0.94272964, 0.91476591)\n",
    "    True Negative: (0.99304671, 0.55297533) \n",
    "    \n",
    "F1 Score: \n",
    "\n",
    "    True Positive: 0.96722596\n",
    "    True Negative: 0.68928087\n",
    "\n",
    "|  Conf. Matrix |  True  | False |\n",
    "| ------------- |:------:| -----:|\n",
    "| predicted +ve |  20280 | 0.142 |\n",
    "| predicted -ve |  1232  | 0.955 |\n",
    " \n",
    "Thus, though accuracy is high, F1 score and false negatives indicates, the classifier is incorrectly classifying data with labels 1 (attacks).\n",
    "\n",
    "### Data pruning \n",
    "Along with given methods of data pruning, following two methods were explored\n",
    "1. Punctuation Removal\n",
    "2. Stopwords Removal\n",
    "3. Stemming\n",
    "\n",
    "#### Punctuations  Removal\n",
    "Removing all the punctuations from the data. This did not significantly improve the accuracy and f1 score.\n",
    "\n",
    "#### Stopwords Removal\n",
    "Words such as 'is', 'the', 'a' are fairly common in english language. Hence, there is a fair chance that these words might not contirbute to the overall efficieny of the algorithm. Sklearn count vectorizer's stopwords removal was used.\n",
    "\n",
    "Test ROC AUC: 0.951\n",
    "\n",
    "Precison and recall:\n",
    "\n",
    "    True Positive: (0.94141386, 0.92160494)\n",
    "    True Negative: (0.99378122, 0.54172714) \n",
    "    \n",
    "F1 Score: \n",
    "\n",
    "    True Positive: 0.96688899\n",
    "    True Negative: 0.68235832\n",
    "\n",
    "|  Conf. Matrix |  True  | False |\n",
    "| ------------- |:------:| -----:|\n",
    "| predicted +ve |  20295 | 127   |\n",
    "| predicted -ve |  1263  | 1493  |\n",
    "\n",
    "Result: The accuracy decreased along with F1 score. Hence, it was not used further. Stop words from nltk included words with negative connotation, hence a customized list of stopwords was prepared and used. But it neither yield any significant results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords=[\"the\", \"a\", \"all\", \"am\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"being\",\n",
    "#            \"between\", \"both\", \"by\", \"did\", \"do\", \"does\", \"doing\", \"during\",  \"for\", \"from\",\n",
    "#            \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\",\n",
    "#            \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
    "#            \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"my\", \"myself\", \"of\", \"other\", \"our\", \"ours\", \"ourselves\",\n",
    "#            \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"that\", \"that's\", \"the\", \"their\", \"about\", \"again\",\n",
    "#            \"theirs\", \"them\", \"themselves\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\",\n",
    "#            \"they've\", \"this\", \"those\", \"through\", \"to\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\",\n",
    "#            \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"who\", \"who's\", \"whom\", \"why\", \"why's\",\n",
    "#            \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
    "#\n",
    "# def preprocess(x):\n",
    "#     x = re.sub('[^a-z\\s\\:\\)\\(]', '', x.lower())                  # get rid of punctuations\n",
    "#     x = [w for w in x.split() if w not in set(stopwords)]        # remove stopwords\n",
    "#     return ' '.join(x)\n",
    "#\n",
    "# train_comments['comment'] = train_comments['comment'].apply(preprocess)\n",
    "# Y_train = train_comments['attack']\n",
    "#\n",
    "# test_comments['comment']=test_comments['comment'].apply(preprocess)\n",
    "# Y_test=test_comments['attack']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Stemming \n",
    "Words such as 'lovingly', 'loved' imply same meaning. So these words are reduced to there original word form 'love'. This method potentially helps in coalesing the features and thus enhancing the model performance.\n",
    "\n",
    "Result:\n",
    "\n",
    "Test ROC AUC: 0.959\n",
    "\n",
    "Precison and recall:\n",
    "\n",
    "    True Positive: (0.94424502, 0.90330052)\n",
    "    True Negative: (0.99182254, 0.56603774) \n",
    "    \n",
    "F1 Score: \n",
    "\n",
    "    True Positive: 0.96744919\n",
    "    True Negative: 0.69596253\n",
    "\n",
    "|  Conf. Matrix |  True  | False |\n",
    "| ------------- |:------:| -----:|\n",
    "| predicted +ve |  20295 | 127   |\n",
    "| predicted -ve |  1263  | 1493  |\n",
    "\n",
    " \n",
    "Thus stemming was found useful and hence was used further.\n",
    "\n",
    "#### Combined (Stemming and Stop word removal)\n",
    "A combination of the above two features was carried out on the strawman code.\n",
    "\n",
    "Result:\n",
    "\n",
    "Test ROC AUC: 0.955\n",
    "\n",
    "Precison and recall:\n",
    "\n",
    "    True Positive: (0.94057111, 0.91780822)\n",
    "    True Negative: (0.99353638, 0.53483309) \n",
    "    \n",
    "F1 Score: \n",
    "\n",
    "    True Positive: 0.96632852\n",
    "    True Negative: 0.67583677\n",
    "\n",
    "|  Conf. Matrix |  True  | False |\n",
    "| ------------- |:------:| -----:|\n",
    "| predicted +ve |  20295 | 127   |\n",
    "| predicted -ve |  1263  | 1493  |\n",
    "\n",
    "\n",
    "\n",
    "Result: Hence the combination of stemming and stopwords was not found useful.\n",
    "\n",
    "Alongwith the above two new methods, some combinations of given methods, such as 3-grams, were tried.\n",
    "Following are the results.\n",
    "\n",
    "*Note: These experiments were carried out with strawman code.*\n",
    "\n",
    "| Data Pruning  | F1            | AUC   |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| -             | 0.689204      | 0.957 |\n",
    "| 3 Gram        | 0.687812      | 0.955 |\n",
    "| Stopword      | 0.682358      | 0.951 |\n",
    "| Stemming      | 0.696544      | 0.959 |\n",
    "| Both above    | 0.675837      | 0.955 |\n",
    "| Char 7 gram   | 0.673421      | 0.951 |\n",
    "| Stem + above  | 0.683660      | 0.949 |\n",
    "\n",
    "3 Grams is a feature extraction procedure, rather than just data pruning. But it has been included it here for comparison.\n",
    "\n",
    "### Feature Extraction\n",
    "Along with Count Vectorization and TF-IDF vectorization already given, following features were used.\n",
    "\n",
    "#### Data grams\n",
    "As mentioned in the document, following combination of n_grams was carried out\n",
    "\n",
    "1. (1, 1)\n",
    "2. (1, 2)\n",
    "3. (1, 3)\n",
    "4. 2\n",
    "5. (1, 2)\n",
    "6. 3\n",
    "\n",
    "The ranges indicate that all the gram formats from 1...n were used. With extensive experimentation it was found that (1, 2) resulted in best accuracy.\n",
    "\n",
    "#### Char versus Word\n",
    "Used characters as features, instead of words, **with n_grams (1, 2)**\n",
    "\n",
    "Result:\n",
    "\n",
    "Test ROC AUC: 0.915\n",
    "\n",
    "Precison and recall:\n",
    "\n",
    "    True Positive: (0.92754089, 0.83380481)\n",
    "    True Negative: (0.9884928, 0.4277939) \n",
    "    \n",
    "F1 Score: \n",
    "\n",
    "    True Positive: 0.95704736\n",
    "    True Negative: 0.56546763\n",
    "\n",
    "|  Conf. Matrix |  True  | False |\n",
    "| ------------- |:------:| -----:|\n",
    "| predicted +ve |  20187 | 235   |\n",
    "| predicted -ve |  1577  | 1179  |\n",
    "\n",
    "**with n_grams (1, 5)**\n",
    "\n",
    "Result:\n",
    "\n",
    "Test ROC AUC: 0.957\n",
    "\n",
    "Precison and recall:\n",
    "\n",
    "    True Positive: (0.94138395, 0.91538933)\n",
    "    True Negative: (0.99324258, 0.54172714) \n",
    "    \n",
    "F1 Score: \n",
    "\n",
    "    True Positive: 0.96661822\n",
    "    True Negative: 0.69064737\n",
    "\n",
    "|  Conf. Matrix |  True  | False |\n",
    "| ------------- |:------:| -----:|\n",
    "| predicted +ve |  20284 | 138   |\n",
    "| predicted -ve |  1263  | 1493  |\n",
    "\n",
    "From the above experiments, it is clear that characters as features with n_grams ()1, 5 do yield improvement.\n",
    "Hence, it was used in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MLPClassifier()),\n",
    "    ])\n",
    "feature_params={\n",
    "    'vect__max_features': (5000, 10000, 30000, 50000),\n",
    "    'vect__ngram_range': ((1, 5)),\n",
    "    'vect__tokenizer': (StemTokenizer()),\n",
    "    'vect__analyzer': ('char'),\n",
    "    'tfidf__norm': (['l2'])\n",
    "}                        #Didn't try with l1 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some non-text features in data such as\n",
    "\n",
    "1. year\n",
    "2. is_logged\n",
    "\n",
    "#### Year\n",
    "Year as a feature could be important, as vocabulary of general spoken english has changed in the past few years. This would mean certain new words or short-handed words are introduced. Thus, separating the newer comments (with newer words) could be useful.\n",
    "\n",
    "Normalisaion of data: It was found that tfidf vectors of the matrix were ranging from 0.01 to 0.99. But the year as a feature is in format 2xxx which is in thousands.Hence scaling of data was applied and year numbers were scaled down in the following fashion-\n",
    "\n",
    "(year % 2000)/100\n",
    "\n",
    "Other techniques such as np.processing were also applied.\n",
    "\n",
    "Result:\n",
    "\n",
    "Test ROC AUC: 0.701\n",
    "\n",
    "Precison and recall:\n",
    "\n",
    "    True Positive: (0.88616349, 0.19274681)\n",
    "    True Negative: (0.94114191, 0.10413643) \n",
    "    \n",
    "F1 Score: \n",
    "\n",
    "    True Positive: 0.91282563\n",
    "    True Negative: 0.1352179\n",
    "\n",
    "|  Conf. Matrix |  True  | False |\n",
    "| ------------- |:------:| -----:|\n",
    "| predicted +ve |  19220 | 1202  |\n",
    "| predicted -ve |  2469  | 287   |\n",
    "\n",
    "Here, it is clear that year as feature yielded bad results. Hence, it was not used further.\n",
    "\n",
    "#### Is_logged\n",
    "Is_logged value could generally separate genuine users, who wish to give constructive comments versus fake users who want to poke fun at others. Hence, this could be good feature. \n",
    "\n",
    "Normalisation of data : It is in binary format. Hence it is normalised using the formula using preprocessing.scale from python, which finds z-scores.\n",
    "\n",
    "Result:\n",
    "\n",
    "Test ROC AUC: 0.722\n",
    "\n",
    "Precison and recall:\n",
    "\n",
    "    True Positive: (0.9127871, 0.1465798)\n",
    "    True Negative: (0.99324258, 0.54172714) \n",
    "    \n",
    "F1 Score: \n",
    "\n",
    "    True Positive: 0.9127871\n",
    "    True Negative: 0.1465798\n",
    "\n",
    "|  Conf. Matrix |  True  | False |\n",
    "| ------------- |:------:| -----:|\n",
    "| predicted +ve |  19195 | 1227  |\n",
    "| predicted -ve |  2441  | 315   |\n",
    "\n",
    "Here, it is clear that similar to year as a feature is_logged did not improve the baseline. hence, this feature was dropped.\n",
    "\n",
    "\n",
    "### Models\n",
    "Experiments were carried out using the following models-\n",
    "1. Multinomial Naive based\n",
    "2. Support Vector machine\n",
    "3. MLP classifier\n",
    "\n",
    "#### Multinomial Naive Bayes\n",
    "Multinomial Navie Bayes algorithm with regularization and fit_priori hyperparameters tuning yielded following results. GridSearchCV was used to get the best parameters.\n",
    "\n",
    "Performing grid search...\n",
    "pipeline: ['vect', 'tfidf', 'clf']\n",
    "parameters:\n",
    "{'clf__alpha': [1e-06, 1e-04],\n",
    " 'clf__fit_prior': (True, False),\n",
    " 'tfidf__norm': ['l2'],\n",
    " 'vect_analyzer': ('word', 'char')\n",
    " 'vect__max_features': (10000, 30000),\n",
    " 'vect__ngram_range': ((1, 2),(1, 5),),\n",
    " 'vect__tokenizer': (<stemmer.StemTokenizer object at 0x126a2a898>,)}\n",
    "done in 29770.999s\n",
    "\n",
    "Best score: 0.927\n",
    "Best parameters set:\n",
    "clf__alpha: 1e-06\n",
    "clf__fit_prior: True\n",
    "tfidf__norm: 'l2'\n",
    "vect__max_features: 10000\n",
    "vect__ngram_range: (1, 2)\n",
    "vect__tokenizer: <stemmer.StemTokenizer object at 0x126a2a898>\n",
    "\n",
    "Test ROC AUC: 0.929\n",
    "\n",
    "F1 score 0.672893\n",
    "\n",
    "\n",
    "Thus, accuracy and F1 score is less than baseline parameters.  \n",
    "\n",
    "#### Support Vector Machine\n",
    "Support vector Machines with linear kernel and balanced class weight, yielded following result. Due to imbalanced data as 'balanced' class weight were used. This would help SVM train better as it would penalize the cost function in accordance to the weight.\n",
    "\n",
    "('vect', CountVectorizer(max_features = 10000, ngram_range = (1,5), analyzer='char', tokenizer=StemTokenizer())),\n",
    "('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "('clf', SVC(C=0.001, class_weight='balanced', kernel='linear') )\n",
    "\n",
    "Test ROC AUC: 0.960\n",
    "\n",
    "F1 Score: 0.72196821\n",
    "\n",
    "F1 score and accuracy have improved from the baseline.\n",
    "\n",
    "\n",
    "#### MLP classifier\n",
    "Multi-Layer Perceptron algorithm was applied and intial results showed improvement good improvement over baseline results. Hence, MLP hyper-parameters were tuned to get better result.\n",
    "\n",
    "Initial result \n",
    "('vect', CountVectorizer(max_features = 10000, ngram_range = (1,5), analyzer='char', # stopwords=stopwords, tokenizer=StemTokenizer())),\n",
    "('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "('clf', MLPClassifier(hidden_layer_sizes=(50,), activation='logistic', alpha=0.01, n_iter_no_change=4, batch_size=200))\n",
    "])\n",
    "\n",
    "Test ROC AUC: 0.960\n",
    "\n",
    "F1 Score: 0.72750698\n",
    "\n",
    "\n",
    "### Hyper-Parameters Optimization\n",
    "\n",
    "Following hyper-parameters were tuned in MLP perceptron model.\n",
    "\n",
    "#### C (alpha) Regularization constant \n",
    "Used alpha in the range of (0.0000005, 0.00002, 0.001, 0.01).\n",
    "Among these 0.001 resulted in the best performance.\n",
    "\n",
    "#### Activation function\n",
    "Applied following activation functions ('logistic', 'tanh', 'relu'). It was found that logistic function gave the best F1 score (precision and recall). Tanh, Relu (which is the default) actually decreased the accuracy.\n",
    "\n",
    "#### Hidden Layers\n",
    "Following hidden layer combinations were used ((50,), (50, 50), (100, ), (50, 50, 50))\n",
    "A network with (50,) hidden neruons was found to give best results in terms of F1 score.\n",
    "\n",
    "#### Max Iteration\n",
    "100, 200, 400 max iteration values were used among which 200 was the best estimate. For some of the combinations though 200 resulted in convergence warning. For these, other hyper parameter combinations with 400 max iteration did not yield significant improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "done in 67309.999s\n",
       " \n",
       "Best score: 0.961\n",
       "Best parameters set:\n",
       "\tclf__alpha: '1e-02'\n",
       "\tclf__hidden_layer_sizes: (50,)\n",
       "\tclf__max_iter: 200\n",
       "\tclf__batch_size: 200\n",
       "\tclf__activation: 'logistic'\n",
       "\tvect__max_features: 10000\n",
       "\tvect__ngram_range: (1, 5)\n",
       "\tvect__tokenizer: <stemmer.StemTokenizer object at 0x126a2a898>\n",
       "\tvect__analyzer: 'char'\n",
       "\ttfidf__norm: 'l2'\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_params={\n",
    "    'clf__alpha': (0.0000005, 0.00002, 0.001, 0.01),\n",
    "    'clf__hidden_layer_sizes': ((50,), (50, 50), (100, ), (50, 50, 50)),\n",
    "    'clf__max_iter': (200, 400),\n",
    "    'clf__batch_size': (200,),\n",
    "    'clf__activation': (['logistic', 'tanh', 'relu'])\n",
    "}\n",
    "\n",
    "parameters={}\n",
    "parameters.update(hyper_params)\n",
    "parameters.update(feature_params)\n",
    "\n",
    "grid_search = GridSearchCV(pipeline,\n",
    "                           parameters,\n",
    "                           cv=5,                    #KFold KFoldStratified\n",
    "                           n_jobs=-1,\n",
    "                           refit=True,\n",
    "                           scoring='f1_weighted')\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "grid_search.fit(train_comments['comment'], train_comments['attack'])\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Cross vaildation\n",
    "#### Metrics\n",
    "Rather than accuracy, precision and recall were better estimates. The data was **unbalanced** and hence vey high accuracy might indicate that the classifiers is very good at predicting one of the classes, but not the other. On the otherhand precision and recall is better estimate as it separates True Postives from False positives and same for thr negatives.\n",
    "\n",
    "#### Cross Validation \n",
    "KFold cross vaildation was used where k = 5. Cross-validation was conducted by using built in GridSearchCV method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Test ROC AUC: 0.961\n",
       "F1 score 0.727\n",
       "array([[20239, 183],\n",
       "[1075, 1681]])\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result_proba = grid_search.predict_proba(test_comments['comment'])\n",
    "test_result = grid_search.predict(test_comments['comment'])\n",
    "\n",
    "auc = roc_auc_score(test_comments['attack'], test_result_proba[:, 1])\n",
    "print('Test ROC AUC: %.3f' % auc)\n",
    "\n",
    "fscore = f1_score(test_comments['attack'], test_result)\n",
    "print('F1 score %3f' % fscore)\n",
    "\n",
    "matrix = confusion_matrix(Y_test, test_pred)\n",
    "print('Confusion Matrix', matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Results\n",
    "\n",
    "Accuracy increased by 0.4%\n",
    "\n",
    "F1 score for true negatives increased by 4%\n",
    "\n",
    "### Interesting things learned from the project\n",
    "1. I got to dive deep into several methods of machine learning. It was interesting to see how hyper-parameters would affect the result impactfully. \n",
    "2. Along, with the above methods, I tried Keras a deeplearning method based upon CNN. In Keras a word is mapped into a 32 bit vector and semantically closer words are mapped closer in the vector space. Such a classifier can carryout  several meaningfull derivations of sentences. \n",
    "\n",
    "### Difficult part of the project\n",
    "1. Tuning hyper-parameters as to tackle the imbalance of the data was a difficult learning curve for me. \n",
    "1. Also, some of the know methods such as 'stopwords' didn't have significant impact, which was difficult to understand.\n",
    "1. Running GridSearchCV on Jupyter notebook is very time consuming. I had to run my code separately on the machine.\n",
    "1. Implementing feature union (trying out Columnar Transforms) was difficult, as it resulted in several python errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
